<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Wang Rui - Research CV</title>
    <style>
        body { font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; line-height: 1.6; color: #333; max-width: 900px; margin: 0 auto; padding: 40px 20px; background-color: #f9f9f9; }
        .container { background-color: #fff; padding: 40px; border-radius: 8px; box-shadow: 0 4px 6px rgba(0,0,0,0.1); }
        h1 { color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 10px; margin-bottom: 20px; }
        h2 { color: #2980b9; margin-top: 30px; border-bottom: 1px solid #eee; padding-bottom: 5px; }
        h3 { color: #34495e; margin-bottom: 5px; }
        p { margin: 5px 0; }
        .contact-info { margin-bottom: 30px; display: flex; flex-wrap: wrap; gap: 15px; font-size: 0.95em; }
        .contact-info a { color: #3498db; text-decoration: none; }
        .contact-info a:hover { text-decoration: underline; }
        .section-item { margin-bottom: 20px; }
        .date-location { font-size: 0.9em; color: #7f8c8d; font-style: italic; margin-bottom: 10px; }
        ul { padding-left: 20px; margin-top: 5px; }
        li { margin-bottom: 5px; }
    </style>
</head>
<body>
    <div class="container">
        <header>
            [cite_start]<h1>Wang Rui - Research CV [cite: 3]</h1>
            <div class="contact-info">
                [cite_start]<span><strong>Birth:</strong> 30th October 1993 [cite: 1, 4]</span>
                [cite_start]<span><strong>Email:</strong> <a href="mailto:rui.wang@g.sp.m.is.nagoya-u.ac.jp">rui.wang@g.sp.m.is.nagoya-u.ac.jp [cite: 8, 10]</a></span>
                [cite_start]<span><strong>Languages:</strong> Chinese, Japanese, English [cite: 7, 10]</span>
                [cite_start]<span><strong>Scholar:</strong> <a href="https://scholar.google.com/citations?user=N3UBXW8AAAAJ&hl=en&authuser=2" target="_blank">Google Scholar Profile [cite: 11, 14]</a></span>
                [cite_start]<span><strong>Linkedin:</strong> <a href="https://www.linkedin.com/in/rui-wang-aa2b0619b/" target="_blank">rui-wang-aa2b0619b [cite: 12, 13, 15]</a></span>
            </div>
        </header>

        <section>
            [cite_start]<h2>Summary [cite: 16]</h2>
            [cite_start]<p>I have several years of research experience in speech signal processing, focusing on spatial hearing and speech enhancement in challenging environments. [cite: 17] [cite_start]At JAIST, I worked on monaural 3D sound localization using HRTF features under Prof. Masashi Unoki. [cite: 18] [cite_start]For my Ph.D. at Nagoya University with Prof. Tomoki Toda, my main topic was directional target speaker extraction (TSE) in noisy and underdetermined conditions, resulting in publications such as TASLP. [cite: 18, 19] [cite_start]My future goal is to extend statistical signal processing (such as independent/low-rank and spatial covariance modeling) by coupling it with DNN priors and latest LLM-based context, aiming for identifiable, sample-efficient, and real-time/low-latency streaming speech enhancement. [cite: 20]</p>
        </section>

        <section>
            [cite_start]<h2>Tools & Skills [cite: 9]</h2>
            [cite_start]<p><strong>Programming:</strong> Python, Shell, C#, Matlab [cite: 6]</p>
            [cite_start]<p><strong>Research Areas:</strong> Spatial audio, Speech signal processing, Speech enhancement/separation, Target speaker extraction, Deep learning [cite: 2, 5]</p>
        </section>

        <section>
            [cite_start]<h2>Work Experience [cite: 21]</h2>
            <div class="section-item">
                [cite_start]<h3>Midea Group, AI Research Institute [cite: 34]</h3>
                <p class="date-location">2025.5- | [cite_start]Shanghai, China [cite: 31, 33]</p>
                [cite_start]<p><strong>Research Engineer:</strong> Research on robust multi-task speech interaction system in challenge environments [cite: 32, 35]</p>
            </div>
            <div class="section-item">
                [cite_start]<h3>Nippon Telegraph and Telephone Corporation (NTT), CS lab [cite: 29]</h3>
                <p class="date-location">2022.3-2022.4 | [cite_start]Tokyo, Japan [cite: 26, 28]</p>
                [cite_start]<p><strong>Winter internship:</strong> Research on robust speech separation [cite: 27, 30]</p>
            </div>
            <div class="section-item">
                [cite_start]<h3>National Institute of Information and Communications Technology (NICT), ASTREC [cite: 25]</h3>
                <p class="date-location">2021.8-2021.10 | [cite_start]Kyoto, Japan [cite: 22, 24]</p>
                [cite_start]<p><strong>Summer internship:</strong> Research on robust speech recognition [cite: 23, 25]</p>
            </div>
        </section>

        <section>
            [cite_start]<h2>Education & Research [cite: 36]</h2>
            <div class="section-item">
                [cite_start]<h3>Nagoya University [cite: 56]</h3>
                <p class="date-location">2021.4-2025.3 | [cite_start]Nagoya, Japan [cite: 53, 55]</p>
                [cite_start]<p><strong>Doctor's degree:</strong> Computer Science, focus on target speaker extraction in challenge environments [cite: 54, 58]</p>
                [cite_start]<p><em>Toda Laboratory of speech [cite: 57]</em></p>
            </div>
            <div class="section-item">
                [cite_start]<h3>Japan Advanced Institute of Science and Technology (JAIST) [cite: 50]</h3>
                <p class="date-location">2018.10-2021.3 | [cite_start]Ishikawa, Japan [cite: 47, 49]</p>
                [cite_start]<p><strong>Master's degree:</strong> Computer Science, focus on HRTF-based DOA estimation and spatial hearing [cite: 48, 52]</p>
                [cite_start]<p><em>Akagi & Unoki Laboratory of speech [cite: 51]</em></p>
            </div>
            <div class="section-item">
                [cite_start]<h3>National Institute of Metrology, China [cite: 45]</h3>
                <p class="date-location">2016.9-2018.8 | [cite_start]Beijing, China [cite: 42, 44]</p>
                [cite_start]<p><strong>Master's course:</strong> Fluid Mechanics (Dropout due to lack of interest) [cite: 43, 46]</p>
            </div>
            <div class="section-item">
                [cite_start]<h3>China Jiliang University [cite: 40]</h3>
                <p class="date-location">2012.9-2016.6 | [cite_start]Hangzhou, China [cite: 37, 39]</p>
                [cite_start]<p><strong>BS degree:</strong> Measurement and Control Technology and Instruments [cite: 38, 41]</p>
            </div>
        </section>

        <section>
            [cite_start]<h2>Publications & Awards [cite: 59, 89]</h2>
            <h3>Journal Papers</h3>
            <ul>
                <li>[2024] R. Wang, T. Fujimura, and T. Toda, "Target Speaker Extraction under Noisy Underdetermined Conditions Using Conditional Variational Autoencoder, Global Style Token, and Neural Postfilter," APSIPA Transactions on Signal and Information Processing, Vol. [cite_start]14, No. 1, e2, pp. 1-26, Jan. 2025. [cite: 63, 68, 69]</li>
                <li>[2024] R. Wang, L. Li, T. Toda, "Dual-channel target speaker extraction based on conditional variational autoencoder and directional information," IEEE/ACM Transactions on Audio, Speech and Language Processing, Vol. [cite_start]32, pp. 1968-1979, Mar. 2024. [cite: 62, 66, 67]</li>
                [cite_start]<li>[2023] R. Wang, B. N. Khanh, D. Morikawa, and M. Unoki, "Method of estimating three dimensional direction-of-arrival based on monaural modulation spectrum," Applied Acoustics, 203, 109215, 9 pages, Feb. 2023. [cite: 61, 65]</li>
            </ul>

            <h3>Conference Papers</h3>
            <ul>
                <li>[2023] R. Wang, T. Toda, "Directional target speaker extraction under noisy underdetermined conditions through conditional variational autoencoder with global style tokens," Proc. [cite_start]IEEE WASPAA, New Paltz, USA, Oct. 2023, pp. 1-5. [cite: 74, 79, 80]</li>
                <li>[2022] R. Wang, L. Li, and T. Tomoki, "Direction-aware target speaker extraction with a dual-channel system based on conditional variational autoencoders under underdetermined conditions," in Proc. IEEE Asia-Pacific Signal Inf. Process. Assoc. Annu. [cite_start]Summit Conf., 2022, pp. 347-354. [cite: 73, 77, 78]</li>
                <li>[2021] N. Li, L. Wang, M. Unoki, S. Li, R. Wang, Me. [cite_start]Ge, J. Dang, "Robust Voice Activity Detection Using a Masked Auditory Encoder Based Convolutional Neural Network," in 2021 IEEE ICASSP, pp. 6828-6832, Jun. 2021. [cite: 64, 70, 71]</li>
                [cite_start]<li>[2021] R. Wang, B. N. Khanh, D. Morikawa, and M. Unoki, "Method of Estimating 3D DOA based on Monaural Modulation Spectrum," In: 2021 RISP NCSP, pp. 137-140, Mar. 2021. [cite: 72, 76]</li>
            </ul>

            <h3>Domestic Papers</h3>
            <ul>
                [cite_start]<li>[2022] R. Wang, Li Li, and T. Toda, "Direction-aware target speaker extraction with conditional variational autoencoders and its sensitivity to direction-of-arrival error," 日本音学会春季研究表会演文集, 2-2-6, pp. 195-196, Sep. 2022. [cite: 83, 87]</li>
                <li>[2022] R. Wang, L. Li, T. Toda, "Target speaker extraction based on conditional variational autoencoder and directional information in underdetermined condition", Technical Report of IEICE, Vol. [cite_start]121, No. 383, EA2021-76, pp. 76-81, Mar. 2022. [cite: 82, 85, 86]</li>
                [cite_start]<li>[2021] R. Wang, B. N. Khanh, D. Morikawa, and M. Unoki, "Method of estimating DOA based on monaural modulation spectrum," 日本音学会春季研究表会演文集, 3-1-21, pp. 321-324, Mar. 2021. [cite: 75, 81]</li>
            </ul>

            [cite_start]<h3>Under Review [cite: 96]</h3>
            <ul>
                [cite_start]<li>[2025] R. Wang et al., "End-to-End Direction-Aware Keyword Spotting with Spatial Priors in Noisy Environments," ICASSP 2026. [cite: 97, 101]</li>
            </ul>

            [cite_start]<h3>Awards [cite: 84, 90, 92, 94]</h3>
            <ul>
                [cite_start]<li>[2023] IEEE WASPAA 2023 Travel Grants. [cite: 95, 100]</li>
                [cite_start]<li>[2022] Acoustical Society of Japan (ASJ)-Student paper award. [cite: 93, 99]</li>
                [cite_start]<li>[2021] Acoustical Society of Japan (ASJ)-Student paper award (Hokuriku branch). [cite: 91, 98]</li>
                [cite_start]<li>[2021] RISP International Workshop on Nonlinear Circuits, Communications and Signal Processing (NCSP)-Student paper award. [cite: 84, 88]</li>
            </ul>
        </section>
    </div>
</body>
</html>
